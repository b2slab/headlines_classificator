{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.utils import np_utils, plot_model\n",
    "import pickle\n",
    "from classes import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from unidecode import unidecode\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import graphviz\n",
    "import time\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#para usar solo una GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf \n",
    "from keras.backend.tensorflow_backend import set_session \n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "set_session(tf.Session(config = config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the pickle file with the dictionary Journal - List of headlines\n",
    "#f = open('titulares_creados/headlines_final.pckl', 'rb')\n",
    "f = open('merged_headlines.pckl', 'rb')\n",
    "lista_periodicos = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating the Headlines object from the pickle file\n",
    "ep = 'http://ep00.epimg.net/rss/elpais/portada.xml'\n",
    "em = 'http://estaticos.elmundo.es/elmundo/rss/portada.xml'\n",
    "lv = 'http://www.lavanguardia.com/mvc/feed/rss/home.xml'\n",
    "lr = 'http://www.larazon.es/rss/portada.xml'\n",
    "abc = 'http://www.abc.es/rss/feeds/abcPortada.xml'\n",
    "\n",
    "urls = [ep, em, lv, lr, abc]\n",
    "journals = ['El Pais', 'El Mundo', 'La Vanguardia', 'La Razon', 'ABC']\n",
    "\n",
    "data = Headlines(periodicos=journals, urls=urls, titulares=lista_periodicos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 183206 entries, 0 to 183205\n",
      "Data columns (total 2 columns):\n",
      "Headline    183206 non-null object\n",
      "Journal     183206 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "#Creating dataframe from the headlines\n",
    "periodicos_df = Headlines.dataframing_headlines(data)\n",
    "periodicos_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Pais 21406\n",
      "El Mundo 60946\n",
      "La Vanguardia 68746\n",
      "La Razon 11026\n",
      "ABC 21082\n"
     ]
    }
   ],
   "source": [
    "# How many headlines there are of each journal?\n",
    "# We should limit the number of headlines in each journal to the minimum number available for one of the classes\n",
    "min_number = Headlines.min_hl_number(data, periodicos_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mafias serbias gitanas compran a niñas para qu...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 normas para que tu hijo esté siempre seguro...</td>\n",
       "      <td>La Vanguardia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La Justicia alemana pide a España que concrete...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Supervivientes: Desvelado el jugador del Real ...</td>\n",
       "      <td>La Vanguardia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fonsi y Daddy Yankee critican la  \"propaganda\"...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Economía multa a Pwc por las cuentas del Popul...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>El City amenaza a Tebas con acciones legales</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ildefonso Falcones: \"Llevo cuatro años y medio...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Economía advierte: se «ralentiza» el PIB por l...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>El criador español que triunfa en Hollywood pi...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Carolina Marín: «Fuera de la pista no suelo ch...</td>\n",
       "      <td>La Razon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>El ministro del Interior amenaza con dimitir y...</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>El espectro de Paulino lleva cuchillo</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Francia venderá 63 cazas Mirage fuera de uso p...</td>\n",
       "      <td>La Razon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Societat Civil alerta de que el jefe de los Mo...</td>\n",
       "      <td>La Razon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>El 'falso médico', al juez: \"Me denuncian por ...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>¿Quiénes son los artistas de éxito relegados a...</td>\n",
       "      <td>La Vanguardia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ERC apuesta por movilizar a los ciudadanos en ...</td>\n",
       "      <td>La Vanguardia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Los jubilados se rebelan contra los recortes d...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Tres tristes puntos para el Barça</td>\n",
       "      <td>La Razon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Padrón saldrá hoy de prisión tras pagar su fia...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>“¿Presos políticos? Es una banalización y falt...</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Críticas al Gobernador de Canadá por tocar a l...</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>La Policía acaba el registro en Economia y det...</td>\n",
       "      <td>La Vanguardia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Así son las 11 playas que llevarán por primera...</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>La aparición de nuevos narcopisos pone en guar...</td>\n",
       "      <td>La Vanguardia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Los hosteleros y remeros afectados por las obr...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Dos jóvenes de Madrid denuncian abusos sexuale...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Òmnium pide a sus socios duplicar la cuota anu...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Francia denuncia fraude en el etiquetado del v...</td>\n",
       "      <td>La Razon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55100</th>\n",
       "      <td>La revolución de los porteros</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55101</th>\n",
       "      <td>Los desahucios caen un 8,4% en el segundo trim...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55102</th>\n",
       "      <td>Sánchez conocía la investigación a Borrell al ...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55103</th>\n",
       "      <td>Especial: los Juegos de México 68</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55104</th>\n",
       "      <td>La trayectoria del huracán Irma</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55105</th>\n",
       "      <td>\"Lo saben por el PSOE, tras los procesos de pr...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55106</th>\n",
       "      <td>La gente es muy normal</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55107</th>\n",
       "      <td>La oficina de denuncias del distrito de Salama...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55108</th>\n",
       "      <td>La queja del Málaga: \"Es grotesco, el balón sa...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55109</th>\n",
       "      <td>La UE impone embargo de armas a Venezuela y  s...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55110</th>\n",
       "      <td>El juzgado penal se hace cargo del caso de Jua...</td>\n",
       "      <td>La Razon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55111</th>\n",
       "      <td>Nadal, en busca del ‘rodillo’ perdido</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55112</th>\n",
       "      <td>GP de Qatar: Dovizioso bate a Márquez en el pr...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55113</th>\n",
       "      <td>La integración, el reto que definirá Alemania</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55114</th>\n",
       "      <td>La Tomatina de Buñol 2017, en directo</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55115</th>\n",
       "      <td>Puigdemont vota por primera vez de forma deleg...</td>\n",
       "      <td>La Vanguardia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55116</th>\n",
       "      <td>La Policía registra la consejería de Economía ...</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55117</th>\n",
       "      <td>Guerra civil talibán en la 'provincia española...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55118</th>\n",
       "      <td>El padre de un mosso asesinado por ETA calific...</td>\n",
       "      <td>La Razon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55119</th>\n",
       "      <td>Francia aprueba subir su presupuesto militar 1...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55120</th>\n",
       "      <td>Los tribunales superiores denuncian el cerco a...</td>\n",
       "      <td>La Vanguardia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55121</th>\n",
       "      <td>Otro sudoku para las Ventanas: con Chacho y pe...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55122</th>\n",
       "      <td>Un teléfono, una chica muerta... El misterio q...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55123</th>\n",
       "      <td>La Generalitat destinará 300.000 euros a la tr...</td>\n",
       "      <td>La Razon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55124</th>\n",
       "      <td>El FC Barcelona podría llegar como campeón al ...</td>\n",
       "      <td>La Vanguardia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55125</th>\n",
       "      <td>La mafia detrás de la diáspora venezolana</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55126</th>\n",
       "      <td>Claves para circular en un atasco</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55127</th>\n",
       "      <td>Mélani García, de 10 años, ganadora de la fina...</td>\n",
       "      <td>El Mundo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55128</th>\n",
       "      <td>La Banca Privada d'Andorra obvió los controles...</td>\n",
       "      <td>El Pais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55129</th>\n",
       "      <td>Detienen a un segundo hombre por el ataque ant...</td>\n",
       "      <td>ABC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55130 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Headline        Journal\n",
       "0      Mafias serbias gitanas compran a niñas para qu...            ABC\n",
       "1      10 normas para que tu hijo esté siempre seguro...  La Vanguardia\n",
       "2      La Justicia alemana pide a España que concrete...       El Mundo\n",
       "3      Supervivientes: Desvelado el jugador del Real ...  La Vanguardia\n",
       "4      Fonsi y Daddy Yankee critican la  \"propaganda\"...       El Mundo\n",
       "5      Economía multa a Pwc por las cuentas del Popul...            ABC\n",
       "6           El City amenaza a Tebas con acciones legales       El Mundo\n",
       "7      Ildefonso Falcones: \"Llevo cuatro años y medio...       El Mundo\n",
       "8      Economía advierte: se «ralentiza» el PIB por l...            ABC\n",
       "9      El criador español que triunfa en Hollywood pi...       El Mundo\n",
       "10     Carolina Marín: «Fuera de la pista no suelo ch...       La Razon\n",
       "11     El ministro del Interior amenaza con dimitir y...        El Pais\n",
       "12                 El espectro de Paulino lleva cuchillo        El Pais\n",
       "13     Francia venderá 63 cazas Mirage fuera de uso p...       La Razon\n",
       "14     Societat Civil alerta de que el jefe de los Mo...       La Razon\n",
       "15     El 'falso médico', al juez: \"Me denuncian por ...       El Mundo\n",
       "16     ¿Quiénes son los artistas de éxito relegados a...  La Vanguardia\n",
       "17     ERC apuesta por movilizar a los ciudadanos en ...  La Vanguardia\n",
       "18     Los jubilados se rebelan contra los recortes d...            ABC\n",
       "19                     Tres tristes puntos para el Barça       La Razon\n",
       "20     Padrón saldrá hoy de prisión tras pagar su fia...       El Mundo\n",
       "21     “¿Presos políticos? Es una banalización y falt...        El Pais\n",
       "22     Críticas al Gobernador de Canadá por tocar a l...        El Pais\n",
       "23     La Policía acaba el registro en Economia y det...  La Vanguardia\n",
       "24     Así son las 11 playas que llevarán por primera...        El Pais\n",
       "25     La aparición de nuevos narcopisos pone en guar...  La Vanguardia\n",
       "26     Los hosteleros y remeros afectados por las obr...            ABC\n",
       "27     Dos jóvenes de Madrid denuncian abusos sexuale...            ABC\n",
       "28     Òmnium pide a sus socios duplicar la cuota anu...            ABC\n",
       "29     Francia denuncia fraude en el etiquetado del v...       La Razon\n",
       "...                                                  ...            ...\n",
       "55100                      La revolución de los porteros        El Pais\n",
       "55101  Los desahucios caen un 8,4% en el segundo trim...            ABC\n",
       "55102  Sánchez conocía la investigación a Borrell al ...       El Mundo\n",
       "55103                  Especial: los Juegos de México 68        El Pais\n",
       "55104                    La trayectoria del huracán Irma        El Pais\n",
       "55105  \"Lo saben por el PSOE, tras los procesos de pr...       El Mundo\n",
       "55106                             La gente es muy normal        El Pais\n",
       "55107  La oficina de denuncias del distrito de Salama...            ABC\n",
       "55108  La queja del Málaga: \"Es grotesco, el balón sa...       El Mundo\n",
       "55109  La UE impone embargo de armas a Venezuela y  s...       El Mundo\n",
       "55110  El juzgado penal se hace cargo del caso de Jua...       La Razon\n",
       "55111              Nadal, en busca del ‘rodillo’ perdido        El Pais\n",
       "55112  GP de Qatar: Dovizioso bate a Márquez en el pr...       El Mundo\n",
       "55113      La integración, el reto que definirá Alemania        El Pais\n",
       "55114              La Tomatina de Buñol 2017, en directo        El Pais\n",
       "55115  Puigdemont vota por primera vez de forma deleg...  La Vanguardia\n",
       "55116  La Policía registra la consejería de Economía ...        El Pais\n",
       "55117  Guerra civil talibán en la 'provincia española...       El Mundo\n",
       "55118  El padre de un mosso asesinado por ETA calific...       La Razon\n",
       "55119  Francia aprueba subir su presupuesto militar 1...            ABC\n",
       "55120  Los tribunales superiores denuncian el cerco a...  La Vanguardia\n",
       "55121  Otro sudoku para las Ventanas: con Chacho y pe...       El Mundo\n",
       "55122  Un teléfono, una chica muerta... El misterio q...       El Mundo\n",
       "55123  La Generalitat destinará 300.000 euros a la tr...       La Razon\n",
       "55124  El FC Barcelona podría llegar como campeón al ...  La Vanguardia\n",
       "55125          La mafia detrás de la diáspora venezolana        El Pais\n",
       "55126                  Claves para circular en un atasco            ABC\n",
       "55127  Mélani García, de 10 años, ganadora de la fina...       El Mundo\n",
       "55128  La Banca Privada d'Andorra obvió los controles...        El Pais\n",
       "55129  Detienen a un segundo hombre por el ataque ant...            ABC\n",
       "\n",
       "[55130 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_per_df = Headlines.keeping_min_headlines(data, periodicos_df, min_number)\n",
    "min_per_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtaining concantenated headlines in order to build the dictionary\n",
    "concatenated_hl = Headlines.concatenate_headlines(data, df = min_per_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary contains 34995 words\n"
     ]
    }
   ],
   "source": [
    "# Building dictionary from headlines\n",
    "sequence_hl = Text_Sequence(concatenated_hl)\n",
    "dictionary_hl, len_dict = Text_Sequence.creating_dict(sequence_hl)\n",
    "print ('The dictionary contains', len_dict, 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([21472, 30581, 16835, 9046, 2119, 23901, 25051, 27658, 29472, 13444, 34434]),\n",
       "       list([1491, 24024, 25051, 27658, 33232, 17767, 14500, 30747, 30381, 13444, 13097, 8598]),\n",
       "       list([20389, 20105, 3382, 25984, 2119, 14269, 27658, 9179, 20389, 2700, 10762, 21637, 9678, 27550]),\n",
       "       ...,\n",
       "       list([22355, 16531, 10762, 1491, 4009, 16484, 10762, 20389, 15746, 10762, 20389, 34556, 20250]),\n",
       "       list([20389, 5694, 27080, 10612, 24245, 21251, 9748, 4046, 9115, 24317]),\n",
       "       list([12056, 2119, 33423, 30373, 17909, 26523, 13097, 5063, 4022, 13097, 24926, 10762, 6720])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping headlines to integer\n",
    "x_int = Headlines.headlines_to_int(data, min_per_df, dictionary_hl)\n",
    "x_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 1, ..., 1, 0, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labelling journal names \n",
    "y_int = Headlines.y_to_int(data, min_per_df)\n",
    "y_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 1, 2, 1, 4, 1, 1, 4, 1, 3, 0, 0, 3, 3, 1, 2, 2, 4, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_int[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_onehot = np_utils.to_categorical(y_int)\n",
    "y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest headline consists of 38 words\n"
     ]
    }
   ],
   "source": [
    "# In Keras, sequences must have the same length\n",
    "# Searching for the longest length of headlines\n",
    "max_headline_lenght = Headlines.max_hl_length(data, x_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ..., 29472, 13444, 34434],\n",
       "       [    0,     0,     0, ..., 13444, 13097,  8598],\n",
       "       [    0,     0,     0, ..., 21637,  9678, 27550],\n",
       "       ...,\n",
       "       [    0,     0,     0, ..., 20389, 34556, 20250],\n",
       "       [    0,     0,     0, ...,  4046,  9115, 24317],\n",
       "       [    0,     0,     0, ..., 24926, 10762,  6720]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#truncate and pad input sequences\n",
    "x_samelength = sequence.pad_sequences(x_int, maxlen=max_headline_lenght)\n",
    "x_samelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Dividing between training and test set\n",
    "#x_train, x_test, y_train, y_test = Headlines.splitting_data(data, x_samelength, y_onehot, 0.1)\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = Headlines.splitting_data_threesets(data, x_samelength, y_onehot, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emebddings shape is (100004, 64)\n"
     ]
    }
   ],
   "source": [
    "# Loading a pre-trained Spanish embedding \n",
    "words, embeddings = pickle.load(open('polyglot-es.pkl', 'rb'), encoding='latin1')\n",
    "print(\"Emebddings shape is {}\".format(embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining Adam optimizer\n",
    "epochss = 1000\n",
    "learning_rate = 5e-4\n",
    "decay_rate = learning_rate/epochss\n",
    "adamm = Adam(lr=learning_rate, beta_1=0.1, beta_2=0.001, epsilon=1e-08, decay=decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the LSTM model\n",
    "def baseline_model():\n",
    "#first layer: embedded layer. uses 5 length vectors to represent each word\n",
    "    embedding_vector_length = 64\n",
    "    model=Sequential()\n",
    "    \n",
    "    #model.add(Embedding(len_dict, embedding_vector_length, input_length=max_headline_lenght))\n",
    "    model.add(Embedding(100004, embedding_vector_length, trainable =False, weights= [embeddings], input_length=max_headline_lenght))\n",
    "    \n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Conv1D(filters=embedding_vector_length, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "#Next layer: LSTM layer with 100 memory units\n",
    "    model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
    "    \n",
    "#Final layer: Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "#Because we used a one-hot encoding for our iris dataset, the output layer must create 3 output values, one for each class.\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#Because it is a binary classification problem, log loss is used as the loss function \n",
    "\n",
    "#Because it is a multi-class classification problem, categorical cross entropy is used as the loss function\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = adamm, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 38, 64)            6400256   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 38, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 38, 64)            12352     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 38, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 6,532,885\n",
      "Trainable params: 132,629\n",
      "Non-trainable params: 6,400,256\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# WAY 2: no sklearn wrapper, simply using keras\n",
    "model = baseline_model()\n",
    "#history = model.fit(x_train, y_train, validation_data= (x_test, y_test), epochs=200, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "dirr = os.path.dirname(os.path.realpath('__file__'))\n",
    "filepath = os.path.join(dirr, 'LSTM_journals','weights-improvement-{epoch:03d}-{val_acc:.4f}.hdf5')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44104 samples, validate on 5513 samples\n",
      "Epoch 1/1000\n",
      "44104/44104 [==============================] - 20s 453us/step - loss: 1.5838 - acc: 0.2616 - val_loss: 1.5748 - val_acc: 0.2752\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.27517, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-001-0.2752.hdf5\n",
      "Epoch 2/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5821 - acc: 0.2634 - val_loss: 1.5819 - val_acc: 0.2663\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/1000\n",
      "44104/44104 [==============================] - 20s 455us/step - loss: 1.5822 - acc: 0.2646 - val_loss: 1.5818 - val_acc: 0.2603\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/1000\n",
      "44104/44104 [==============================] - 20s 456us/step - loss: 1.5795 - acc: 0.2671 - val_loss: 1.5744 - val_acc: 0.2721\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/1000\n",
      "44104/44104 [==============================] - 20s 456us/step - loss: 1.5794 - acc: 0.2673 - val_loss: 1.5733 - val_acc: 0.2723\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5784 - acc: 0.2674 - val_loss: 1.5706 - val_acc: 0.2812\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.27517 to 0.28115, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-006-0.2812.hdf5\n",
      "Epoch 7/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5772 - acc: 0.2701 - val_loss: 1.5696 - val_acc: 0.2737\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5765 - acc: 0.2700 - val_loss: 1.5673 - val_acc: 0.2784\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/1000\n",
      "44104/44104 [==============================] - 20s 462us/step - loss: 1.5755 - acc: 0.2743 - val_loss: 1.5687 - val_acc: 0.2773\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5751 - acc: 0.2703 - val_loss: 1.5647 - val_acc: 0.2779\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5748 - acc: 0.2726 - val_loss: 1.5647 - val_acc: 0.2810\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/1000\n",
      "44104/44104 [==============================] - 20s 462us/step - loss: 1.5716 - acc: 0.2765 - val_loss: 1.5643 - val_acc: 0.2813\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.28115 to 0.28134, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-012-0.2813.hdf5\n",
      "Epoch 13/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5703 - acc: 0.2817 - val_loss: 1.5636 - val_acc: 0.2853\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.28134 to 0.28533, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-013-0.2853.hdf5\n",
      "Epoch 14/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5706 - acc: 0.2751 - val_loss: 1.5613 - val_acc: 0.2846\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5709 - acc: 0.2789 - val_loss: 1.5602 - val_acc: 0.2817\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 16/1000\n",
      "44104/44104 [==============================] - 20s 458us/step - loss: 1.5694 - acc: 0.2781 - val_loss: 1.5579 - val_acc: 0.2837\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 17/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5697 - acc: 0.2789 - val_loss: 1.5577 - val_acc: 0.2822\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 18/1000\n",
      "44104/44104 [==============================] - 20s 458us/step - loss: 1.5688 - acc: 0.2811 - val_loss: 1.5576 - val_acc: 0.2861\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.28533 to 0.28605, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-018-0.2861.hdf5\n",
      "Epoch 19/1000\n",
      "44104/44104 [==============================] - 20s 456us/step - loss: 1.5686 - acc: 0.2807 - val_loss: 1.5561 - val_acc: 0.2853\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 20/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5665 - acc: 0.2803 - val_loss: 1.5530 - val_acc: 0.2850\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 21/1000\n",
      "44104/44104 [==============================] - 20s 453us/step - loss: 1.5670 - acc: 0.2819 - val_loss: 1.5559 - val_acc: 0.2851\n",
      "\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 22/1000\n",
      "44104/44104 [==============================] - 20s 454us/step - loss: 1.5650 - acc: 0.2841 - val_loss: 1.5533 - val_acc: 0.2828\n",
      "\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 23/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5680 - acc: 0.2831 - val_loss: 1.5519 - val_acc: 0.2866\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.28605 to 0.28660, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-023-0.2866.hdf5\n",
      "Epoch 24/1000\n",
      "44104/44104 [==============================] - 20s 456us/step - loss: 1.5660 - acc: 0.2844 - val_loss: 1.5524 - val_acc: 0.2841\n",
      "\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 25/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5660 - acc: 0.2823 - val_loss: 1.5525 - val_acc: 0.2835\n",
      "\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 26/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5648 - acc: 0.2819 - val_loss: 1.5492 - val_acc: 0.2875\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.28660 to 0.28750, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-026-0.2875.hdf5\n",
      "Epoch 27/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5658 - acc: 0.2845 - val_loss: 1.5490 - val_acc: 0.2968\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.28750 to 0.29675, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-027-0.2968.hdf5\n",
      "Epoch 28/1000\n",
      "44104/44104 [==============================] - 20s 458us/step - loss: 1.5633 - acc: 0.2870 - val_loss: 1.5491 - val_acc: 0.2906\n",
      "\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 29/1000\n",
      "44104/44104 [==============================] - 20s 462us/step - loss: 1.5656 - acc: 0.2832 - val_loss: 1.5494 - val_acc: 0.2866\n",
      "\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 30/1000\n",
      "44104/44104 [==============================] - 20s 454us/step - loss: 1.5645 - acc: 0.2869 - val_loss: 1.5485 - val_acc: 0.2870\n",
      "\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 31/1000\n",
      "44104/44104 [==============================] - 20s 458us/step - loss: 1.5647 - acc: 0.2853 - val_loss: 1.5464 - val_acc: 0.2908\n",
      "\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 32/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5640 - acc: 0.2883 - val_loss: 1.5511 - val_acc: 0.2890\n",
      "\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 33/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5653 - acc: 0.2874 - val_loss: 1.5483 - val_acc: 0.2899\n",
      "\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 34/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5620 - acc: 0.2864 - val_loss: 1.5473 - val_acc: 0.2890\n",
      "\n",
      "Epoch 00034: val_acc did not improve\n",
      "Epoch 35/1000\n",
      "44104/44104 [==============================] - 20s 462us/step - loss: 1.5637 - acc: 0.2847 - val_loss: 1.5473 - val_acc: 0.2886\n",
      "\n",
      "Epoch 00035: val_acc did not improve\n",
      "Epoch 36/1000\n",
      "44104/44104 [==============================] - 20s 458us/step - loss: 1.5632 - acc: 0.2864 - val_loss: 1.5481 - val_acc: 0.2888\n",
      "\n",
      "Epoch 00036: val_acc did not improve\n",
      "Epoch 37/1000\n",
      "44104/44104 [==============================] - 20s 458us/step - loss: 1.5629 - acc: 0.2866 - val_loss: 1.5477 - val_acc: 0.2873\n",
      "\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 38/1000\n",
      "44104/44104 [==============================] - 20s 456us/step - loss: 1.5632 - acc: 0.2853 - val_loss: 1.5459 - val_acc: 0.2890\n",
      "\n",
      "Epoch 00038: val_acc did not improve\n",
      "Epoch 39/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5638 - acc: 0.2892 - val_loss: 1.5467 - val_acc: 0.2893\n",
      "\n",
      "Epoch 00039: val_acc did not improve\n",
      "Epoch 40/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5634 - acc: 0.2862 - val_loss: 1.5473 - val_acc: 0.2960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_acc did not improve\n",
      "Epoch 41/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5625 - acc: 0.2906 - val_loss: 1.5458 - val_acc: 0.2931\n",
      "\n",
      "Epoch 00041: val_acc did not improve\n",
      "Epoch 42/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5615 - acc: 0.2900 - val_loss: 1.5476 - val_acc: 0.2935\n",
      "\n",
      "Epoch 00042: val_acc did not improve\n",
      "Epoch 43/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5609 - acc: 0.2887 - val_loss: 1.5473 - val_acc: 0.2902\n",
      "\n",
      "Epoch 00043: val_acc did not improve\n",
      "Epoch 44/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5621 - acc: 0.2928 - val_loss: 1.5455 - val_acc: 0.2924\n",
      "\n",
      "Epoch 00044: val_acc did not improve\n",
      "Epoch 45/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5627 - acc: 0.2883 - val_loss: 1.5465 - val_acc: 0.2917\n",
      "\n",
      "Epoch 00045: val_acc did not improve\n",
      "Epoch 46/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5609 - acc: 0.2879 - val_loss: 1.5421 - val_acc: 0.2933\n",
      "\n",
      "Epoch 00046: val_acc did not improve\n",
      "Epoch 47/1000\n",
      "44104/44104 [==============================] - 20s 454us/step - loss: 1.5585 - acc: 0.2916 - val_loss: 1.5443 - val_acc: 0.2924\n",
      "\n",
      "Epoch 00047: val_acc did not improve\n",
      "Epoch 48/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5598 - acc: 0.2932 - val_loss: 1.5419 - val_acc: 0.2960\n",
      "\n",
      "Epoch 00048: val_acc did not improve\n",
      "Epoch 49/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5579 - acc: 0.2920 - val_loss: 1.5449 - val_acc: 0.2942\n",
      "\n",
      "Epoch 00049: val_acc did not improve\n",
      "Epoch 50/1000\n",
      "44104/44104 [==============================] - 20s 463us/step - loss: 1.5623 - acc: 0.2888 - val_loss: 1.5449 - val_acc: 0.2861\n",
      "\n",
      "Epoch 00050: val_acc did not improve\n",
      "Epoch 51/1000\n",
      "44104/44104 [==============================] - 20s 455us/step - loss: 1.5592 - acc: 0.2925 - val_loss: 1.5418 - val_acc: 0.2964\n",
      "\n",
      "Epoch 00051: val_acc did not improve\n",
      "Epoch 52/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5572 - acc: 0.2937 - val_loss: 1.5428 - val_acc: 0.2935\n",
      "\n",
      "Epoch 00052: val_acc did not improve\n",
      "Epoch 53/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5587 - acc: 0.2891 - val_loss: 1.5449 - val_acc: 0.2906\n",
      "\n",
      "Epoch 00053: val_acc did not improve\n",
      "Epoch 54/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5583 - acc: 0.2921 - val_loss: 1.5418 - val_acc: 0.3024\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.29675 to 0.30238, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-054-0.3024.hdf5\n",
      "Epoch 55/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5611 - acc: 0.2909 - val_loss: 1.5428 - val_acc: 0.2980\n",
      "\n",
      "Epoch 00055: val_acc did not improve\n",
      "Epoch 56/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5562 - acc: 0.2949 - val_loss: 1.5418 - val_acc: 0.2913\n",
      "\n",
      "Epoch 00056: val_acc did not improve\n",
      "Epoch 57/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5569 - acc: 0.2953 - val_loss: 1.5396 - val_acc: 0.3000\n",
      "\n",
      "Epoch 00057: val_acc did not improve\n",
      "Epoch 58/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5578 - acc: 0.2943 - val_loss: 1.5424 - val_acc: 0.2971\n",
      "\n",
      "Epoch 00058: val_acc did not improve\n",
      "Epoch 59/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5560 - acc: 0.2944 - val_loss: 1.5401 - val_acc: 0.2986\n",
      "\n",
      "Epoch 00059: val_acc did not improve\n",
      "Epoch 60/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5570 - acc: 0.2966 - val_loss: 1.5407 - val_acc: 0.2969\n",
      "\n",
      "Epoch 00060: val_acc did not improve\n",
      "Epoch 61/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5608 - acc: 0.2931 - val_loss: 1.5410 - val_acc: 0.2946\n",
      "\n",
      "Epoch 00061: val_acc did not improve\n",
      "Epoch 62/1000\n",
      "44104/44104 [==============================] - 20s 455us/step - loss: 1.5566 - acc: 0.2951 - val_loss: 1.5405 - val_acc: 0.2951\n",
      "\n",
      "Epoch 00062: val_acc did not improve\n",
      "Epoch 63/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5572 - acc: 0.2931 - val_loss: 1.5401 - val_acc: 0.2982\n",
      "\n",
      "Epoch 00063: val_acc did not improve\n",
      "Epoch 64/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5575 - acc: 0.2934 - val_loss: 1.5391 - val_acc: 0.2995\n",
      "\n",
      "Epoch 00064: val_acc did not improve\n",
      "Epoch 65/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5541 - acc: 0.2935 - val_loss: 1.5388 - val_acc: 0.2989\n",
      "\n",
      "Epoch 00065: val_acc did not improve\n",
      "Epoch 66/1000\n",
      "44104/44104 [==============================] - 20s 465us/step - loss: 1.5551 - acc: 0.2952 - val_loss: 1.5377 - val_acc: 0.2986\n",
      "\n",
      "Epoch 00066: val_acc did not improve\n",
      "Epoch 67/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5554 - acc: 0.2949 - val_loss: 1.5396 - val_acc: 0.2986\n",
      "\n",
      "Epoch 00067: val_acc did not improve\n",
      "Epoch 68/1000\n",
      "44104/44104 [==============================] - 20s 458us/step - loss: 1.5562 - acc: 0.2953 - val_loss: 1.5387 - val_acc: 0.2977\n",
      "\n",
      "Epoch 00068: val_acc did not improve\n",
      "Epoch 69/1000\n",
      "44104/44104 [==============================] - 20s 456us/step - loss: 1.5573 - acc: 0.2948 - val_loss: 1.5379 - val_acc: 0.3007\n",
      "\n",
      "Epoch 00069: val_acc did not improve\n",
      "Epoch 70/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5562 - acc: 0.2963 - val_loss: 1.5387 - val_acc: 0.2966\n",
      "\n",
      "Epoch 00070: val_acc did not improve\n",
      "Epoch 71/1000\n",
      "44104/44104 [==============================] - 20s 456us/step - loss: 1.5551 - acc: 0.2931 - val_loss: 1.5379 - val_acc: 0.2931\n",
      "\n",
      "Epoch 00071: val_acc did not improve\n",
      "Epoch 72/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5548 - acc: 0.2947 - val_loss: 1.5387 - val_acc: 0.2937\n",
      "\n",
      "Epoch 00072: val_acc did not improve\n",
      "Epoch 73/1000\n",
      "44104/44104 [==============================] - 21s 465us/step - loss: 1.5544 - acc: 0.2970 - val_loss: 1.5387 - val_acc: 0.2980\n",
      "\n",
      "Epoch 00073: val_acc did not improve\n",
      "Epoch 74/1000\n",
      "44104/44104 [==============================] - 20s 455us/step - loss: 1.5552 - acc: 0.2960 - val_loss: 1.5360 - val_acc: 0.3004\n",
      "\n",
      "Epoch 00074: val_acc did not improve\n",
      "Epoch 75/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5565 - acc: 0.2927 - val_loss: 1.5359 - val_acc: 0.2991\n",
      "\n",
      "Epoch 00075: val_acc did not improve\n",
      "Epoch 76/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5532 - acc: 0.2957 - val_loss: 1.5358 - val_acc: 0.2987\n",
      "\n",
      "Epoch 00076: val_acc did not improve\n",
      "Epoch 77/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5564 - acc: 0.2947 - val_loss: 1.5382 - val_acc: 0.2986\n",
      "\n",
      "Epoch 00077: val_acc did not improve\n",
      "Epoch 78/1000\n",
      "44104/44104 [==============================] - 20s 458us/step - loss: 1.5517 - acc: 0.2982 - val_loss: 1.5362 - val_acc: 0.2997\n",
      "\n",
      "Epoch 00078: val_acc did not improve\n",
      "Epoch 79/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5539 - acc: 0.2971 - val_loss: 1.5376 - val_acc: 0.3009\n",
      "\n",
      "Epoch 00079: val_acc did not improve\n",
      "Epoch 80/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5525 - acc: 0.2960 - val_loss: 1.5373 - val_acc: 0.2991\n",
      "\n",
      "Epoch 00080: val_acc did not improve\n",
      "Epoch 81/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5542 - acc: 0.2970 - val_loss: 1.5360 - val_acc: 0.2995\n",
      "\n",
      "Epoch 00081: val_acc did not improve\n",
      "Epoch 82/1000\n",
      "44104/44104 [==============================] - 20s 458us/step - loss: 1.5546 - acc: 0.2962 - val_loss: 1.5362 - val_acc: 0.3035\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.30238 to 0.30346, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-082-0.3035.hdf5\n",
      "Epoch 83/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5533 - acc: 0.2972 - val_loss: 1.5381 - val_acc: 0.2960\n",
      "\n",
      "Epoch 00083: val_acc did not improve\n",
      "Epoch 84/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5528 - acc: 0.2991 - val_loss: 1.5370 - val_acc: 0.2962\n",
      "\n",
      "Epoch 00084: val_acc did not improve\n",
      "Epoch 85/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44104/44104 [==============================] - 20s 458us/step - loss: 1.5528 - acc: 0.2976 - val_loss: 1.5368 - val_acc: 0.2978\n",
      "\n",
      "Epoch 00085: val_acc did not improve\n",
      "Epoch 86/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5525 - acc: 0.2983 - val_loss: 1.5339 - val_acc: 0.3036\n",
      "\n",
      "Epoch 00086: val_acc improved from 0.30346 to 0.30365, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-086-0.3036.hdf5\n",
      "Epoch 87/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5533 - acc: 0.2986 - val_loss: 1.5333 - val_acc: 0.3022\n",
      "\n",
      "Epoch 00087: val_acc did not improve\n",
      "Epoch 88/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5529 - acc: 0.2964 - val_loss: 1.5366 - val_acc: 0.2978\n",
      "\n",
      "Epoch 00088: val_acc did not improve\n",
      "Epoch 89/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5535 - acc: 0.2957 - val_loss: 1.5364 - val_acc: 0.3000\n",
      "\n",
      "Epoch 00089: val_acc did not improve\n",
      "Epoch 90/1000\n",
      "44104/44104 [==============================] - 20s 455us/step - loss: 1.5519 - acc: 0.2966 - val_loss: 1.5361 - val_acc: 0.3060\n",
      "\n",
      "Epoch 00090: val_acc improved from 0.30365 to 0.30600, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-090-0.3060.hdf5\n",
      "Epoch 91/1000\n",
      "44104/44104 [==============================] - 20s 460us/step - loss: 1.5546 - acc: 0.2955 - val_loss: 1.5376 - val_acc: 0.3020\n",
      "\n",
      "Epoch 00091: val_acc did not improve\n",
      "Epoch 92/1000\n",
      "44104/44104 [==============================] - 20s 462us/step - loss: 1.5510 - acc: 0.3000 - val_loss: 1.5318 - val_acc: 0.3093\n",
      "\n",
      "Epoch 00092: val_acc improved from 0.30600 to 0.30927, saving model to /home/angela/repos/headlines_classificator/LSTM_journals/weights-improvement-092-0.3093.hdf5\n",
      "Epoch 93/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5545 - acc: 0.3009 - val_loss: 1.5362 - val_acc: 0.3033\n",
      "\n",
      "Epoch 00093: val_acc did not improve\n",
      "Epoch 94/1000\n",
      "44104/44104 [==============================] - 20s 456us/step - loss: 1.5527 - acc: 0.2966 - val_loss: 1.5344 - val_acc: 0.3049\n",
      "\n",
      "Epoch 00094: val_acc did not improve\n",
      "Epoch 95/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5550 - acc: 0.3008 - val_loss: 1.5358 - val_acc: 0.3033\n",
      "\n",
      "Epoch 00095: val_acc did not improve\n",
      "Epoch 96/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5563 - acc: 0.2962 - val_loss: 1.5334 - val_acc: 0.3006\n",
      "\n",
      "Epoch 00096: val_acc did not improve\n",
      "Epoch 97/1000\n",
      "44104/44104 [==============================] - 20s 456us/step - loss: 1.5534 - acc: 0.2997 - val_loss: 1.5367 - val_acc: 0.3026\n",
      "\n",
      "Epoch 00097: val_acc did not improve\n",
      "Epoch 98/1000\n",
      "44104/44104 [==============================] - 20s 454us/step - loss: 1.5544 - acc: 0.2977 - val_loss: 1.5358 - val_acc: 0.3000\n",
      "\n",
      "Epoch 00098: val_acc did not improve\n",
      "Epoch 99/1000\n",
      "44104/44104 [==============================] - 20s 455us/step - loss: 1.5525 - acc: 0.2998 - val_loss: 1.5341 - val_acc: 0.3058\n",
      "\n",
      "Epoch 00099: val_acc did not improve\n",
      "Epoch 100/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5535 - acc: 0.2992 - val_loss: 1.5356 - val_acc: 0.3007\n",
      "\n",
      "Epoch 00100: val_acc did not improve\n",
      "Epoch 101/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5542 - acc: 0.2974 - val_loss: 1.5334 - val_acc: 0.3046\n",
      "\n",
      "Epoch 00101: val_acc did not improve\n",
      "Epoch 102/1000\n",
      "44104/44104 [==============================] - 20s 453us/step - loss: 1.5544 - acc: 0.2992 - val_loss: 1.5336 - val_acc: 0.2995\n",
      "\n",
      "Epoch 00102: val_acc did not improve\n",
      "Epoch 103/1000\n",
      "44104/44104 [==============================] - 20s 458us/step - loss: 1.5541 - acc: 0.2981 - val_loss: 1.5326 - val_acc: 0.3026\n",
      "\n",
      "Epoch 00103: val_acc did not improve\n",
      "Epoch 104/1000\n",
      "44104/44104 [==============================] - 20s 459us/step - loss: 1.5525 - acc: 0.2989 - val_loss: 1.5308 - val_acc: 0.3040\n",
      "\n",
      "Epoch 00104: val_acc did not improve\n",
      "Epoch 105/1000\n",
      "44104/44104 [==============================] - 20s 454us/step - loss: 1.5541 - acc: 0.2974 - val_loss: 1.5298 - val_acc: 0.3042\n",
      "\n",
      "Epoch 00105: val_acc did not improve\n",
      "Epoch 106/1000\n",
      "44104/44104 [==============================] - 20s 462us/step - loss: 1.5546 - acc: 0.3022 - val_loss: 1.5384 - val_acc: 0.2986\n",
      "\n",
      "Epoch 00106: val_acc did not improve\n",
      "Epoch 107/1000\n",
      "44104/44104 [==============================] - 20s 454us/step - loss: 1.5515 - acc: 0.3021 - val_loss: 1.5320 - val_acc: 0.3020\n",
      "\n",
      "Epoch 00107: val_acc did not improve\n",
      "Epoch 108/1000\n",
      "44104/44104 [==============================] - 20s 456us/step - loss: 1.5546 - acc: 0.3008 - val_loss: 1.5337 - val_acc: 0.3065\n",
      "\n",
      "Epoch 00108: val_acc did not improve\n",
      "Epoch 109/1000\n",
      "44104/44104 [==============================] - 20s 457us/step - loss: 1.5547 - acc: 0.2990 - val_loss: 1.5336 - val_acc: 0.3080\n",
      "\n",
      "Epoch 00109: val_acc did not improve\n",
      "Epoch 110/1000\n",
      "44104/44104 [==============================] - 20s 455us/step - loss: 1.5529 - acc: 0.3021 - val_loss: 1.5332 - val_acc: 0.3009\n",
      "\n",
      "Epoch 00110: val_acc did not improve\n",
      "Epoch 111/1000\n",
      "44104/44104 [==============================] - 20s 461us/step - loss: 1.5575 - acc: 0.2992 - val_loss: 1.5313 - val_acc: 0.2995\n",
      "\n",
      "Epoch 00111: val_acc did not improve\n",
      "Epoch 112/1000\n",
      "19840/44104 [============>.................] - ETA: 10s - loss: 1.5580 - acc: 0.2966"
     ]
    }
   ],
   "source": [
    "#history = model.fit(x_train, y_train, validation_data= (x_test, y_test), epochs=epochss, batch_size=128, callbacks=callbacks_list)\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data= (x_val, y_val), epochs=epochss, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "#filename = \"weights-improvement-440-0.3196.hdf5\"\n",
    "#model.load_weights(filename)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predprob = model.predict(x_test)\n",
    "y_pred = model.predict_classes(x_test)\n",
    "y_pred_decoded = Headlines.int_to_journal(data, y_pred)\n",
    "\n",
    "y_trpred = model.predict_classes(x_train)\n",
    "y_trpred_decoded = Headlines.int_to_journal(data, y_trpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting in the test set\n",
    "#y_pred = estimator.predict(x_test)\n",
    "#y_pred_decoded = Headlines.int_to_journal(data, y_pred)\n",
    "#y_pred = encoder.inverse_transform(predictions)\n",
    "\n",
    "#y_predprob = estimator.predict_proba(x_test)[:,1]\n",
    "#y_trpred = estimator.predict(x_train)\n",
    "#y_trpred_decoded = Headlines.int_to_journal(data, y_trpred)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Both y_train and y_test are one-hot-encoded. Decoding them for model reporting.\n",
    "y_train_tocat = Headlines.onehot_to_categorical(data, y_train)\n",
    "y_test_tocat = Headlines.onehot_to_categorical(data, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print model report:\n",
    "print (\"\\nModel Report\")\n",
    "print (\"Accuracy (train set): %.4g\" % metrics.accuracy_score(y_train_tocat, y_trpred))\n",
    "print (\"Accuracy (test set): %.4g\" % metrics.accuracy_score(y_test_tocat, y_pred))\n",
    "print(\"Confusion matrix:\")\n",
    "print (metrics.confusion_matrix(y_test_tocat, y_pred))\n",
    "print(\"Detailed classification report:\")\n",
    "print (metrics.classification_report(y_test_tocat, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the model with k-Fold Validation\n",
    "seed=7\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x_train, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
